{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word Embedding\n> is a technique used in Natural Language Processing (NLP) to represent words as continuous vectors of real numbers in a lower-dimensional space. These vectors capture the semantic meaning of words, allowing words with similar meanings to have similar vector representations. Word embeddings are learned from large text corpora and enable models to understand the context and relationships between words more effectively than traditional methods like one-hot encoding. Popular word embedding methods include Word2Vec, GloVe, and FastText.","metadata":{}},{"cell_type":"markdown","source":"# Techniques :\n1. Predictive Models: Word2Vec, FastText\n2. Count-based Models: GloVe\n3. Contextualized Models: ELMo, BERT, GPT","metadata":{"execution":{"iopub.status.busy":"2024-09-10T22:20:05.029182Z","iopub.execute_input":"2024-09-10T22:20:05.029731Z","iopub.status.idle":"2024-09-10T22:20:05.037819Z","shell.execute_reply.started":"2024-09-10T22:20:05.029685Z","shell.execute_reply":"2024-09-10T22:20:05.036109Z"}}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\n\n# Sample corpus (list of sentences)\nsentences = [\n    \"I love natural language processing.\",\n    \"Natural language processing is fun.\",\n    \"I enjoy learning about data science.\",\n    \"Machine learning is a part of data science.\"\n]\n\n# Tokenize the sentences\ntokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n\n# Train the Word2Vec model\nmodel_word2vec = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=0)  # CBOW model\n\n# Get the vector for a specific word\nword_vector = model_word2vec.wv['data']\n\n# Print the word vector for 'data' (first 5 dimensions)\nprint(f\"Word Vector for 'data': {word_vector[:5]}\")\n\n# Find similar words to 'data'\nsimilar_words = model_word2vec.wv.most_similar('data')\nprint(\"\\nWords similar to 'data':\", similar_words)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-10T22:22:02.919006Z","iopub.execute_input":"2024-09-10T22:22:02.919522Z","iopub.status.idle":"2024-09-10T22:22:02.945922Z","shell.execute_reply.started":"2024-09-10T22:22:02.919476Z","shell.execute_reply":"2024-09-10T22:22:02.944590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import FastText\nfrom nltk.tokenize import word_tokenize\n\n# Sample corpus (list of sentences)\nsentences = [\n    \"I love natural language processing.\",\n    \"Natural language processing is fun.\",\n    \"I enjoy learning about data science.\",\n    \"Machine learning is a part of data science.\"\n]\n\n# Tokenize the sentences\ntokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n\n# Train the FastText model\nmodel_fasttext = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1)\n\n# Get the vector for a specific word\nword_vector_fasttext = model_fasttext.wv['data']\n\n# Print the word vector for 'data' (first 5 dimensions)\nprint(f\"FastText Word Vector for 'data': {word_vector_fasttext[:5]}\")\n\n# Find similar words to 'data'\nsimilar_words_fasttext = model_fasttext.wv.most_similar('data')\nprint(\"\\nWords similar to 'data' using FastText:\", similar_words_fasttext)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T22:22:50.363829Z","iopub.execute_input":"2024-09-10T22:22:50.364319Z","iopub.status.idle":"2024-09-10T22:22:51.973482Z","shell.execute_reply.started":"2024-09-10T22:22:50.364275Z","shell.execute_reply":"2024-09-10T22:22:51.972333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}