{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9205961,"sourceType":"datasetVersion","datasetId":5566243}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentence & Word Tokenization In NLTK","metadata":{}},{"cell_type":"code","source":"pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:16.813374Z","iopub.execute_input":"2024-08-30T22:10:16.813772Z","iopub.status.idle":"2024-08-30T22:10:31.890859Z","shell.execute_reply.started":"2024-08-30T22:10:16.813731Z","shell.execute_reply":"2024-08-30T22:10:31.889211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:31.894995Z","iopub.execute_input":"2024-08-30T22:10:31.895503Z","iopub.status.idle":"2024-08-30T22:10:31.901617Z","shell.execute_reply.started":"2024-08-30T22:10:31.895455Z","shell.execute_reply":"2024-08-30T22:10:31.900448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_tokenize(\"Mr. Ahmed likes pizza from Italy. Maria loves sushi from Tokyo!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:31.903054Z","iopub.execute_input":"2024-08-30T22:10:31.903421Z","iopub.status.idle":"2024-08-30T22:10:32.045765Z","shell.execute_reply.started":"2024-08-30T22:10:31.903367Z","shell.execute_reply":"2024-08-30T22:10:32.044577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nword_tokenize(\"Mr. Ahmed likes pizza from Italy. Maria loves sushi from Tokyo!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:32.047336Z","iopub.execute_input":"2024-08-30T22:10:32.047791Z","iopub.status.idle":"2024-08-30T22:10:32.058980Z","shell.execute_reply.started":"2024-08-30T22:10:32.047752Z","shell.execute_reply":"2024-08-30T22:10:32.057815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Sentence Tokenization In Spacy","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:24:37.809720Z","iopub.execute_input":"2024-08-19T21:24:37.810399Z","iopub.status.idle":"2024-08-19T21:24:37.814627Z","shell.execute_reply.started":"2024-08-19T21:24:37.810362Z","shell.execute_reply":"2024-08-19T21:24:37.813503Z"}}},{"cell_type":"code","source":"pip install spacy","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:32.061058Z","iopub.execute_input":"2024-08-30T22:10:32.061508Z","iopub.status.idle":"2024-08-30T22:10:47.419842Z","shell.execute_reply.started":"2024-08-30T22:10:32.061463Z","shell.execute_reply":"2024-08-30T22:10:47.418432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:47.422776Z","iopub.execute_input":"2024-08-30T22:10:47.423661Z","iopub.status.idle":"2024-08-30T22:10:50.598726Z","shell.execute_reply.started":"2024-08-30T22:10:47.423594Z","shell.execute_reply":"2024-08-30T22:10:50.597488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")#Loads SpaCy's small English language model.\n\ndoc = nlp(\"Mr. Ahmed likes pizza from Italy. Maria loves sushi from Tokyo!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:50.600344Z","iopub.execute_input":"2024-08-30T22:10:50.600902Z","iopub.status.idle":"2024-08-30T22:10:52.311251Z","shell.execute_reply.started":"2024-08-30T22:10:50.600869Z","shell.execute_reply":"2024-08-30T22:10:52.310092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.315501Z","iopub.execute_input":"2024-08-30T22:10:52.315850Z","iopub.status.idle":"2024-08-30T22:10:52.323427Z","shell.execute_reply.started":"2024-08-30T22:10:52.315821Z","shell.execute_reply":"2024-08-30T22:10:52.322194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sentence in doc.sents: #The 'doc.sents' attribute contains the individual sentences as detected by SpaCy's sentence boundary detection.\n    print(sentence)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.324741Z","iopub.execute_input":"2024-08-30T22:10:52.325087Z","iopub.status.idle":"2024-08-30T22:10:52.335405Z","shell.execute_reply.started":"2024-08-30T22:10:52.325051Z","shell.execute_reply":"2024-08-30T22:10:52.334107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Word Tokenization","metadata":{}},{"cell_type":"code","source":"for sentence in doc.sents:\n    for word in sentence:\n        print(word)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.337094Z","iopub.execute_input":"2024-08-30T22:10:52.337579Z","iopub.status.idle":"2024-08-30T22:10:52.350659Z","shell.execute_reply.started":"2024-08-30T22:10:52.337537Z","shell.execute_reply":"2024-08-30T22:10:52.349441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Collecting email ids of students from students information sheet using spacy","metadata":{}},{"cell_type":"code","source":"# Open the specified file from the Kaggle input directory and read all lines into a list\n\nwith open(\"/kaggle/input/test-txt/dayton_high_school_students.txt\") as f:\n    text = f.readlines()\ntext","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.352368Z","iopub.execute_input":"2024-08-30T22:10:52.353226Z","iopub.status.idle":"2024-08-30T22:10:52.370415Z","shell.execute_reply.started":"2024-08-30T22:10:52.353183Z","shell.execute_reply":"2024-08-30T22:10:52.369220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join all lines from the list into a single string with spaces between them\n\ntext = \" \".join(text)\ntext","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.371934Z","iopub.execute_input":"2024-08-30T22:10:52.372269Z","iopub.status.idle":"2024-08-30T22:10:52.380889Z","shell.execute_reply.started":"2024-08-30T22:10:52.372242Z","shell.execute_reply":"2024-08-30T22:10:52.379495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(text)\nemails = []\nfor token in doc:\n    if token.like_email:\n        emails.append(token.text)\nemails ","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.382620Z","iopub.execute_input":"2024-08-30T22:10:52.383088Z","iopub.status.idle":"2024-08-30T22:10:52.430723Z","shell.execute_reply.started":"2024-08-30T22:10:52.383048Z","shell.execute_reply":"2024-08-30T22:10:52.429583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract URLS from text ","metadata":{}},{"cell_type":"code","source":"text='''\nLook for data to help you address the question. Governments are good\nsources because data from public research is often freely available. Good\nplaces to start include http://www.data.gov/, and http://www.science.\ngov/, and in the United Kingdom, http://data.gov.uk/.\nTwo of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \nand the European Social Survey at http://www.europeansocialsurvey.org/.'''","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.432181Z","iopub.execute_input":"2024-08-30T22:10:52.432566Z","iopub.status.idle":"2024-08-30T22:10:52.438368Z","shell.execute_reply.started":"2024-08-30T22:10:52.432535Z","shell.execute_reply":"2024-08-30T22:10:52.437164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(text)\nurl = []\nfor token in doc:\n    if token.like_url:\n        url.append(token.text)\nurl","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.439783Z","iopub.execute_input":"2024-08-30T22:10:52.440178Z","iopub.status.idle":"2024-08-30T22:10:52.482214Z","shell.execute_reply.started":"2024-08-30T22:10:52.440142Z","shell.execute_reply":"2024-08-30T22:10:52.481071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Extract all money transaction from below sentence along with currency.","metadata":{}},{"cell_type":"code","source":"transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.483555Z","iopub.execute_input":"2024-08-30T22:10:52.483867Z","iopub.status.idle":"2024-08-30T22:10:52.489449Z","shell.execute_reply.started":"2024-08-30T22:10:52.483841Z","shell.execute_reply":"2024-08-30T22:10:52.488129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(transactions)\nfor token in doc:\n    if token.like_num and doc[token.i+1].is_currency:\n        print(token.text, doc[token.i+1].text) \n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.491027Z","iopub.execute_input":"2024-08-30T22:10:52.491442Z","iopub.status.idle":"2024-08-30T22:10:52.515444Z","shell.execute_reply.started":"2024-08-30T22:10:52.491398Z","shell.execute_reply":"2024-08-30T22:10:52.514137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lowercasing\n> Converting all text to lowercase to reduce variability in words (e.g., \"Apple\" and \"apple\" treated the same).","metadata":{}},{"cell_type":"code","source":"text = \"This is an Example with Mixed CASES.\"\ndoc=nlp(text)\nlowercased_text = doc.text.lower()\nprint(lowercased_text)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.517111Z","iopub.execute_input":"2024-08-30T22:10:52.517589Z","iopub.status.idle":"2024-08-30T22:10:52.538868Z","shell.execute_reply.started":"2024-08-30T22:10:52.517546Z","shell.execute_reply":"2024-08-30T22:10:52.537904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatization/Stemming \n> is the process of reducing a word to its base or dictionary form, known as the lemma. This process considers the word’s part of speech and the context in which it is used.","metadata":{"execution":{"iopub.status.busy":"2024-08-20T21:23:38.204026Z","iopub.execute_input":"2024-08-20T21:23:38.204743Z","iopub.status.idle":"2024-08-20T21:23:38.210930Z","shell.execute_reply.started":"2024-08-20T21:23:38.204710Z","shell.execute_reply":"2024-08-20T21:23:38.209809Z"}}},{"cell_type":"code","source":"doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\ndoc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\nfor token in doc:\n    print(token.lemma_)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.540738Z","iopub.execute_input":"2024-08-30T22:10:52.541189Z","iopub.status.idle":"2024-08-30T22:10:52.572719Z","shell.execute_reply.started":"2024-08-30T22:10:52.541151Z","shell.execute_reply":"2024-08-30T22:10:52.571639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\ndoc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\nfor token in doc:\n    print(token, \" | \", token.lemma_)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:52.574227Z","iopub.execute_input":"2024-08-30T22:10:52.574585Z","iopub.status.idle":"2024-08-30T22:10:53.863195Z","shell.execute_reply.started":"2024-08-30T22:10:52.574555Z","shell.execute_reply":"2024-08-30T22:10:53.861913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process the first sentence\ndoc1 = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n# Get lemmatized version of the first sentence\nlemmatized_sentence1 = ' '.join(token.lemma_ for token in doc1)\n\n# Process the second sentence\ndoc2 = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n# Get lemmatized version of the second sentence\nlemmatized_sentence2 = ' '.join(token.lemma_ for token in doc2)\n\n# Print the lemmatized sentences\nprint(\"Lemmatized Sentence 1:\")\nprint(lemmatized_sentence1)\n\nprint(\"\\nLemmatized Sentence 2:\")\nprint(lemmatized_sentence2)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:53.864680Z","iopub.execute_input":"2024-08-30T22:10:53.865058Z","iopub.status.idle":"2024-08-30T22:10:53.893588Z","shell.execute_reply.started":"2024-08-30T22:10:53.865025Z","shell.execute_reply":"2024-08-30T22:10:53.892320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Punctuation Removal\n>  Stripping out punctuation marks to focus on meaningful text.","metadata":{}},{"cell_type":"code","source":"text = \"Hello, world! It's a sunny day. Can you believe it? Wow!\"\n\n# Process the text with SpaCy\ndoc = nlp(text)\n\n# Remove punctuation tokens\ntext_no_punctuation = \" \".join([token.text for token in doc if not token.is_punct])\n\nprint(text_no_punctuation)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:53.894966Z","iopub.execute_input":"2024-08-30T22:10:53.895310Z","iopub.status.idle":"2024-08-30T22:10:53.919641Z","shell.execute_reply.started":"2024-08-30T22:10:53.895280Z","shell.execute_reply":"2024-08-30T22:10:53.918183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part-of-Speech (POS) tagging\nis the process of assigning grammatical categories (e.g., noun, verb, adjective) to each word in a text based on its role in the sentence.\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T22:03:29.035786Z","iopub.execute_input":"2024-08-20T22:03:29.036482Z","iopub.status.idle":"2024-08-20T22:03:29.040740Z","shell.execute_reply.started":"2024-08-20T22:03:29.036447Z","shell.execute_reply":"2024-08-20T22:03:29.039596Z"}}},{"cell_type":"code","source":"# Example text\ntext = \"Natural Language Processing is a fascinating field of study.\"\ndoc = nlp(text)\nprint(\"spaCy POS Tags:\")\nfor token in doc:\n    print(f\"{token.text}: {token.pos_} ({token.tag_})\")","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:53.921649Z","iopub.execute_input":"2024-08-30T22:10:53.922030Z","iopub.status.idle":"2024-08-30T22:10:53.942926Z","shell.execute_reply.started":"2024-08-30T22:10:53.921999Z","shell.execute_reply":"2024-08-30T22:10:53.941183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Stopword Removal\n>  Stopword Removal is the process of eliminating common words from a text that carry little meaning and are often removed to improve the efficiency and accuracy of NLP tasks (e.g., \"the\", \"is\", \"in\").","metadata":{}},{"cell_type":"code","source":"# Example text\ntext = \"Natural Language Processing is an interesting field of study.\"\n\n# Process the text with spaCy\ndoc = nlp(text)\n\n# Remove stopwords\nfiltered_tokens = [token.text for token in doc if not token.is_stop]\n\n# Print the filtered tokens\nprint(\"Tokens after stopword removal:\", filtered_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:10:53.944828Z","iopub.execute_input":"2024-08-30T22:10:53.945610Z","iopub.status.idle":"2024-08-30T22:10:53.965946Z","shell.execute_reply.started":"2024-08-30T22:10:53.945559Z","shell.execute_reply":"2024-08-30T22:10:53.964747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Named Entity Recognition (NER) \n>  is a process in Natural Language Processing (NLP) that identifies and classifies named entities in a text into predefined categories, such as names of persons, organizations, locations, dates, and more.","metadata":{}},{"cell_type":"code","source":"# Example text\ntext = \"Apple Inc. is planning to open a new office in San Francisco in 2024.\"\n\n# Process the text with spaCy\ndoc = nlp(text)\n\n# Print named entities\nprint(\"Named Entities using spaCy:\")\nfor ent in doc.ents:\n    print(ent.text, ent.label_)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T22:13:47.924587Z","iopub.execute_input":"2024-08-30T22:13:47.924993Z","iopub.status.idle":"2024-08-30T22:13:47.946169Z","shell.execute_reply.started":"2024-08-30T22:13:47.924963Z","shell.execute_reply":"2024-08-30T22:13:47.944988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# PARACTICE EXERCISE \n> Sample Text:\n\"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976. The company, headquartered in Cupertino, California, revolutionized personal computing with the Macintosh in 1984. As of 2024, Apple is one of the most valuable companies in the world, with a market capitalization of over $2 trillion.\"\n* Tokenize the sample text into words and sentences using spaCy.\n* Remove stopwords from the tokenized words using spaCy's built-in stopword list.\n* Lemmatize the tokenized words using spaCy to convert them to their base forms.\n* dentify and classify named entities in the text, such as people, organizations, and locations.\n*  Tag each word in the text with its corresponding part of speech.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}