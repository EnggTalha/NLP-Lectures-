{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# One Hot Encoding \n>  One-Hot Encoding is a technique used to represent words as vectors. Each word in the vocabulary is converted into a binary vector with a length equal to the size of the vocabulary.","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Define the Sample Texts","metadata":{"execution":{"iopub.status.busy":"2024-09-03T20:28:45.790687Z","iopub.execute_input":"2024-09-03T20:28:45.791133Z","iopub.status.idle":"2024-09-03T20:28:45.803384Z","shell.execute_reply.started":"2024-09-03T20:28:45.791092Z","shell.execute_reply":"2024-09-03T20:28:45.801912Z"}}},{"cell_type":"code","source":"documents = [\n    \"I love natural language processing\",\n    \"Natural language processing is fascinating\",\n    \"I love programming and data science\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:03:32.511383Z","iopub.execute_input":"2024-09-03T22:03:32.511831Z","iopub.status.idle":"2024-09-03T22:03:32.526648Z","shell.execute_reply.started":"2024-09-03T22:03:32.511789Z","shell.execute_reply":"2024-09-03T22:03:32.525285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Tokenize the Text","metadata":{}},{"cell_type":"code","source":"# Tokenize each document by lowercasing and splitting by spaces\ntokenized_docs = [doc.lower().split() for doc in documents]\n\n# Print each tokenized document\nfor i, tokens in enumerate(tokenized_docs, 1):\n    print(f\"Document {i}: {tokens}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:06:07.902906Z","iopub.execute_input":"2024-09-03T22:06:07.903350Z","iopub.status.idle":"2024-09-03T22:06:07.910387Z","shell.execute_reply.started":"2024-09-03T22:06:07.903308Z","shell.execute_reply":"2024-09-03T22:06:07.909143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Create a Vocabulary","metadata":{}},{"cell_type":"code","source":"all_tokens = [word for tokens in tokenized_docs for word in tokens]\n\n# Unique words (Vocabulary)\nvocabulary = sorted(set(all_tokens))\nvocabulary","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:06:19.361226Z","iopub.execute_input":"2024-09-03T22:06:19.361721Z","iopub.status.idle":"2024-09-03T22:06:19.370067Z","shell.execute_reply.started":"2024-09-03T22:06:19.361676Z","shell.execute_reply":"2024-09-03T22:06:19.368869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten the list of tokenized words into a single list\nall_tokens = [word for tokens in tokenized_docs for word in tokens]\n\n# Unique words (Vocabulary)\nvocabulary = sorted(set(all_tokens))\n\n# Display vocabulary in comma-separated row form\nvocabulary_row = \", \".join(vocabulary)\nvocabulary_row\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:06:25.033818Z","iopub.execute_input":"2024-09-03T22:06:25.034374Z","iopub.status.idle":"2024-09-03T22:06:25.044646Z","shell.execute_reply.started":"2024-09-03T22:06:25.034306Z","shell.execute_reply":"2024-09-03T22:06:25.043364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Create One-Hot Encodings","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize the CountVectorizer for One-Hot Encoding\nvectorizer = CountVectorizer(binary=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:13:14.990184Z","iopub.execute_input":"2024-09-03T22:13:14.990665Z","iopub.status.idle":"2024-09-03T22:13:14.996539Z","shell.execute_reply.started":"2024-09-03T22:13:14.990623Z","shell.execute_reply":"2024-09-03T22:13:14.995254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Fit and transform the documents\none_hot_matrix = vectorizer.fit_transform(documents)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:13:16.299693Z","iopub.execute_input":"2024-09-03T22:13:16.300224Z","iopub.status.idle":"2024-09-03T22:13:16.307203Z","shell.execute_reply.started":"2024-09-03T22:13:16.300173Z","shell.execute_reply":"2024-09-03T22:13:16.305898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to DataFrame\none_hot_df = pd.DataFrame(one_hot_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Display the result\nprint(one_hot_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:13:18.489820Z","iopub.execute_input":"2024-09-03T22:13:18.490258Z","iopub.status.idle":"2024-09-03T22:13:18.500738Z","shell.execute_reply.started":"2024-09-03T22:13:18.490219Z","shell.execute_reply":"2024-09-03T22:13:18.499623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Display the One-Hot Encoded Matrix","metadata":{}},{"cell_type":"code","source":"# Convert to DataFrame\none_hot_df = pd.DataFrame(one_hot_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add a column for document labels\none_hot_df.insert(0, 'Document', [f'Document {i+1}' for i in range(len(documents))])\n\n# Display the DataFrame in tabular form\ndisplay(one_hot_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:13:21.778062Z","iopub.execute_input":"2024-09-03T22:13:21.778498Z","iopub.status.idle":"2024-09-03T22:13:21.797163Z","shell.execute_reply.started":"2024-09-03T22:13:21.778456Z","shell.execute_reply":"2024-09-03T22:13:21.796024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag of Words (BoW)\n Bag of Words is a simple and commonly used technique in NLP that converts text into a matrix of token counts, ignoring the grammar and word order but capturing the frequency of each word in the document","metadata":{}},{"cell_type":"markdown","source":"# Steps to Create a BoW Model\n* Step 1: Tokenize each document.\n* Step 2: Create a vocabulary of all unique words across the documents.\n* Step 3: Count the occurrence of each word in each document.","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Tokenization","metadata":{}},{"cell_type":"code","source":"# Your list of documents\ndocuments = [\n    \"I love natural language processing.\",\n    \"Natural language processing is fascinating.\",\n    \"I love programming and data science.\"\n]\n\n# Tokenize each document by lowercasing and splitting by spaces\ntokenized_docs = [doc.lower().split() for doc in documents]\n\n# Print each tokenized document\nfor i, tokens in enumerate(tokenized_docs, 1):\n    print(f\"Document {i}: {tokens}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:06:37.879589Z","iopub.execute_input":"2024-09-03T22:06:37.880036Z","iopub.status.idle":"2024-09-03T22:06:37.887719Z","shell.execute_reply.started":"2024-09-03T22:06:37.879993Z","shell.execute_reply":"2024-09-03T22:06:37.886534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Combine Tokens","metadata":{}},{"cell_type":"code","source":"# Flatten the list of tokenized words into a single list\nall_tokens = [word for tokens in tokenized_docs for word in tokens]\n\n# Print the combined list of all tokens\nprint(all_tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:06:40.875423Z","iopub.execute_input":"2024-09-03T22:06:40.876442Z","iopub.status.idle":"2024-09-03T22:06:40.882260Z","shell.execute_reply.started":"2024-09-03T22:06:40.876391Z","shell.execute_reply":"2024-09-03T22:06:40.881154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Create a set of unique words (vocabulary)\nvocabulary = set(all_tokens)\nprint(vocabulary)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:06:47.744639Z","iopub.execute_input":"2024-09-03T22:06:47.745037Z","iopub.status.idle":"2024-09-03T22:06:47.751196Z","shell.execute_reply.started":"2024-09-03T22:06:47.745000Z","shell.execute_reply":"2024-09-03T22:06:47.749940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VOCABULARY ","metadata":{}},{"cell_type":"code","source":"# Convert to sorted list\nsorted_vocabulary = sorted(vocabulary)\nprint(sorted_vocabulary)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:06:51.389502Z","iopub.execute_input":"2024-09-03T22:06:51.389947Z","iopub.status.idle":"2024-09-03T22:06:51.396241Z","shell.execute_reply.started":"2024-09-03T22:06:51.389904Z","shell.execute_reply":"2024-09-03T22:06:51.394885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag of Words (BoW)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Create the Bag of Words Model\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)\n\n# Convert to DataFrame for better visualization\nimport pandas as pd\nbow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\nprint(bow_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:14:04.861178Z","iopub.execute_input":"2024-09-03T22:14:04.861660Z","iopub.status.idle":"2024-09-03T22:14:04.874259Z","shell.execute_reply.started":"2024-09-03T22:14:04.861617Z","shell.execute_reply":"2024-09-03T22:14:04.872955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to DataFrame\none_hot_df = pd.DataFrame(one_hot_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add a column for document labels\none_hot_df.insert(0, 'Document', [f'Document {i+1}' for i in range(len(documents))])\n\n# Display the DataFrame in tabular form\ndisplay(one_hot_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:14:18.032755Z","iopub.execute_input":"2024-09-03T22:14:18.033202Z","iopub.status.idle":"2024-09-03T22:14:18.050940Z","shell.execute_reply.started":"2024-09-03T22:14:18.033161Z","shell.execute_reply":"2024-09-03T22:14:18.049661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming X is the vectorized data and vectorizer is your vectorizer object\nbow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Function to add a random background color to each cell\ndef random_color(val):\n    color = \"#{:06x}\".format(np.random.randint(0, 0xFFFFFF))\n    return f'background-color: {color}'\n\n# Apply the random_color function to each cell in the DataFrame\nstyled_bow_df = bow_df.style.applymap(random_color)\n\n# Display the styled DataFrame\nstyled_bow_df","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:07:30.636221Z","iopub.execute_input":"2024-09-03T22:07:30.636678Z","iopub.status.idle":"2024-09-03T22:07:30.743194Z","shell.execute_reply.started":"2024-09-03T22:07:30.636637Z","shell.execute_reply":"2024-09-03T22:07:30.741903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization without stopwords","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Sample Texts\ndocuments = [\n    \"I love natural language processing.\",\n    \"Natural language processing is fascinating.\",\n    \"I love programming and data science.\"\n]\n\n# Create the Bag of Words Model with stop words removal\nvectorizer = CountVectorizer(stop_words='english')\nX = vectorizer.fit_transform(documents)\n\n# Convert to DataFrame for better visualization\nimport pandas as pd\nbow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\nprint(bow_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:07:34.762655Z","iopub.execute_input":"2024-09-03T22:07:34.763059Z","iopub.status.idle":"2024-09-03T22:07:34.775683Z","shell.execute_reply.started":"2024-09-03T22:07:34.763023Z","shell.execute_reply":"2024-09-03T22:07:34.774405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\n# Sample Texts\ndocuments = [\n    \"I love natural language processing.\",\n    \"Natural language processing is fascinating.\",\n    \"I love programming and data science.\"\n]\n\n# Create the Bag of Words Model with stop words removal\nvectorizer = CountVectorizer(stop_words='english')\nX = vectorizer.fit_transform(documents)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:15:25.774189Z","iopub.execute_input":"2024-09-03T22:15:25.774683Z","iopub.status.idle":"2024-09-03T22:15:25.783424Z","shell.execute_reply.started":"2024-09-03T22:15:25.774638Z","shell.execute_reply":"2024-09-03T22:15:25.782015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:07:56.061994Z","iopub.execute_input":"2024-09-03T22:07:56.062434Z","iopub.status.idle":"2024-09-03T22:07:56.069478Z","shell.execute_reply.started":"2024-09-03T22:07:56.062391Z","shell.execute_reply":"2024-09-03T22:07:56.068106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXAMPLE # 02 ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:15:52.267779Z","iopub.execute_input":"2024-09-03T22:15:52.268242Z","iopub.status.idle":"2024-09-03T22:15:52.273585Z","shell.execute_reply.started":"2024-09-03T22:15:52.268198Z","shell.execute_reply":"2024-09-03T22:15:52.272359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"documents = [\n    \"I love programming in Python. Python programming is great.\",\n    \"Python programming is fun. Programming is very interesting.\",\n    \"I love learning new programming languages. Programming is exciting.\"\n]\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:15:53.693739Z","iopub.execute_input":"2024-09-03T22:15:53.694209Z","iopub.status.idle":"2024-09-03T22:15:53.699951Z","shell.execute_reply.started":"2024-09-03T22:15:53.694165Z","shell.execute_reply":"2024-09-03T22:15:53.698547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# . By One Hot Encodeing ","metadata":{"execution":{"iopub.status.busy":"2024-09-03T21:46:34.435471Z","iopub.execute_input":"2024-09-03T21:46:34.435945Z","iopub.status.idle":"2024-09-03T21:46:34.441314Z","shell.execute_reply.started":"2024-09-03T21:46:34.435902Z","shell.execute_reply":"2024-09-03T21:46:34.439803Z"}}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize the CountVectorizer for One-Hot Encoding\nvectorizer = CountVectorizer(binary=True)\n\n# Fit and transform the documents\none_hot_matrix = vectorizer.fit_transform(documents)\n\n# Convert to DataFrame\none_hot_df = pd.DataFrame(one_hot_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Display the result\nprint(one_hot_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:15:54.925385Z","iopub.execute_input":"2024-09-03T22:15:54.926494Z","iopub.status.idle":"2024-09-03T22:15:54.939346Z","shell.execute_reply.started":"2024-09-03T22:15:54.926440Z","shell.execute_reply":"2024-09-03T22:15:54.937946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to DataFrame\none_hot_df = pd.DataFrame(one_hot_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add a column for document labels\none_hot_df.insert(0, 'Document', [f'Document {i+1}' for i in range(len(documents))])\n\n# Display the DataFrame in tabular form\ndisplay(one_hot_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:11:18.159789Z","iopub.execute_input":"2024-09-03T22:11:18.160414Z","iopub.status.idle":"2024-09-03T22:11:18.178843Z","shell.execute_reply.started":"2024-09-03T22:11:18.160364Z","shell.execute_reply":"2024-09-03T22:11:18.177539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # By Bag Of Words : ","metadata":{"execution":{"iopub.status.busy":"2024-09-03T21:46:13.870477Z","iopub.execute_input":"2024-09-03T21:46:13.870938Z","iopub.status.idle":"2024-09-03T21:46:13.875993Z","shell.execute_reply.started":"2024-09-03T21:46:13.870894Z","shell.execute_reply":"2024-09-03T21:46:13.874798Z"}}},{"cell_type":"code","source":"# Define the documents\ndocuments = [\n    \"I love programming in Python. Python programming is great.\",\n    \"Python programming is fun. Programming is very interesting.\",\n    \"I love learning new programming languages. Programming is exciting.\"\n]\n\n# Initialize the CountVectorizer for Bag of Words with frequency counts\nvectorizer = CountVectorizer(binary=False)\n\n# Fit and transform the documents\nbow_matrix = vectorizer.fit_transform(documents)\n\n# Convert to DataFrame\nbow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n# Add a column for document labels\nbow_df.insert(0, 'Document', [f'Document {i+1}' for i in range(len(documents))])\n\n# Display the DataFrame in tabular form\ndisplay(bow_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Other Examples ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Sample documents\ndocuments = [\"I love programming\", \"Programming is fun Programming\", \"I love coding\"]\n\n# Initialize CountVectorizer (default is binary=False, so it counts frequencies)\nvectorizer = CountVectorizer(binary=True)\nX = vectorizer.fit_transform(documents)\n\n# Output results\nprint(\"Feature Names:\", vectorizer.get_feature_names_out())\nprint(\"Frequency Count Matrix:\\n\", X.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:43:58.365203Z","iopub.execute_input":"2024-09-03T22:43:58.365674Z","iopub.status.idle":"2024-09-03T22:43:58.375282Z","shell.execute_reply.started":"2024-09-03T22:43:58.365632Z","shell.execute_reply":"2024-09-03T22:43:58.373959Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Feature Names: ['coding' 'fun' 'is' 'love' 'programming']\nFrequency Count Matrix:\n [[0 0 0 1 1]\n [0 1 1 0 1]\n [1 0 0 1 0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Sample documents\ndocuments = [\"I love programming\", \"Programming is fun Programming\", \"I love coding\"]\n\n# Initialize CountVectorizer (default is binary=False, so it counts frequencies)\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)\n\n# Output results\nprint(\"Feature Names:\", vectorizer.get_feature_names_out())\nprint(\"Frequency Count Matrix:\\n\", X.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T22:43:39.686963Z","iopub.execute_input":"2024-09-03T22:43:39.687428Z","iopub.status.idle":"2024-09-03T22:43:39.696993Z","shell.execute_reply.started":"2024-09-03T22:43:39.687385Z","shell.execute_reply":"2024-09-03T22:43:39.695618Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Feature Names: ['coding' 'fun' 'is' 'love' 'programming']\nFrequency Count Matrix:\n [[0 0 0 1 1]\n [0 1 1 0 2]\n [1 0 0 1 0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# With Text Precessing ","metadata":{}},{"cell_type":"code","source":"# Sample documents with higher frequency words\ndocuments = [\n    \"I love programming programming programming!\",\n    \"Programming is fun fun fun!\",\n    \"I love coding coding coding, especially in Python Python.\",\n    \"Coding in Python Python is great great, and programming programming is fun!\"\n]\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T08:28:54.911553Z","iopub.execute_input":"2024-09-04T08:28:54.912115Z","iopub.status.idle":"2024-09-04T08:28:54.919305Z","shell.execute_reply.started":"2024-09-04T08:28:54.912035Z","shell.execute_reply":"2024-09-04T08:28:54.917480Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nimport re\n\ndef preprocess_text(text):\n    text = text.lower()  # Lowercasing\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    words = text.split()  # Split into words\n    filtered_words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n    return ' '.join(filtered_words)\n# Apply preprocessing to each document\npreprocessed_documents = [preprocess_text(doc) for doc in documents]\n\n# Step 2: One-Hot Encoding\nvectorizer_one_hot = CountVectorizer(binary=True)\nX_one_hot = vectorizer_one_hot.fit_transform(preprocessed_documents)\n\nprint(\"Preprocessed Documents:\")\nfor doc in preprocessed_documents:\n    print(doc)\n\nprint(\"\\nOne-Hot Encoding Feature Names:\", vectorizer_one_hot.get_feature_names_out())\nprint(\"One-Hot Encoding Matrix:\\n\", X_one_hot.toarray())\n\n# Step 3: Bag of Words with Frequency Counts\nvectorizer_bow = CountVectorizer(binary=False)\nX_bow = vectorizer_bow.fit_transform(preprocessed_documents)\n\nprint(\"\\nBag of Words Feature Names:\", vectorizer_bow.get_feature_names_out())\nprint(\"Bag of Words Frequency Count Matrix:\\n\", X_bow.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T08:28:57.284812Z","iopub.execute_input":"2024-09-04T08:28:57.285501Z","iopub.status.idle":"2024-09-04T08:28:57.413817Z","shell.execute_reply.started":"2024-09-04T08:28:57.285440Z","shell.execute_reply":"2024-09-04T08:28:57.411971Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to each document\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m preprocessed_documents \u001b[38;5;241m=\u001b[39m [preprocess_text(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Step 2: One-Hot Encoding\u001b[39;00m\n\u001b[1;32m     15\u001b[0m vectorizer_one_hot \u001b[38;5;241m=\u001b[39m CountVectorizer(binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to each document\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m preprocessed_documents \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Step 2: One-Hot Encoding\u001b[39;00m\n\u001b[1;32m     15\u001b[0m vectorizer_one_hot \u001b[38;5;241m=\u001b[39m CountVectorizer(binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Split into words\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ENGLISH_STOP_WORDS]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n","Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Split into words\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mENGLISH_STOP_WORDS\u001b[49m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n","\u001b[0;31mNameError\u001b[0m: name 'ENGLISH_STOP_WORDS' is not defined"],"ename":"NameError","evalue":"name 'ENGLISH_STOP_WORDS' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# TASK 1\nGiven a set of documents, how can you create a binary representation where each word is represented as a binary feature (1 for presence, 0 for absence)?\n\n**Question**: Write a Python script to perform One-Hot Encoding on the following documents:\n\n1. \"**I love programming in Python.**\"\n1. \"**Python programming is fun.**\"\n1. \"**I love learning new languages.**\"\n\nDisplay the result in a tabular format with rows representing documents and columns representing words.","metadata":{}},{"cell_type":"markdown","source":"# Task 2","metadata":{}},{"cell_type":"markdown","source":"Given a set of documents, create a frequency-based representation where each word is represented by its count in the document. Analyze the frequency of words that appear more than once.\n\n**Question**: Write a Python script to perform Bag of Words with frequency counting on the following documents:\n\n1. \"**Data science is fun. Data science involves statistics and coding.**\"\n1. \"**Coding is an essential skill for data science.**\"\n1. \"**Statistics and data analysis are crucial for data science.**\"\n\nDisplay the result in a tabular format showing the frequency of each word in each document. Make sure to highlight words that appear more than once within each document and across documents.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}